{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from more_itertools import flatten\n",
    "import scipy as sp\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sys import maxint\n",
    "from shuyo.vocabulary import load_file, load_corpus, Vocabulary\n",
    "from shuyo.hdplda2 import HDPLDA\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "minint = -maxint - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Teh et al](http://www.cs.berkeley.edu/%7Ejordan/papers/hierarchical-dp.pdf) run their HDP-LDA sampler on the \"Nematode biology abstracts\" corpus. These abstracts are no longer available where linked in the paper, but I found them [here](https://web.archive.org/web/20040328153507/http://elegans.swmed.edu/wli/cgcbib).\n",
    "\n",
    "The authors don't provide their preprocessed data. They say:\n",
    "\n",
    "> There are 5838 abstracts in total. After removing standard stopwords and words appearing less than 10 times, we are left with 476441 words in total and a vocabulary size of 5699.\n",
    "\n",
    "Below, I process the data and get 5828 abstracts (by removing duplicated citations. Using the stopwords provided by scikit-learn and eliminating words that occur fewer than 10 times, I got 5800 terms and a total of 488903 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to replicate Teh's preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of abstracts 5828\n",
      "col_freq_filter [(False, 19283), (True, 5800)]\n",
      "total words 488903\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/nematode biology abstracts.txt\", \"r\") as f:\n",
    "    raw_abstracts = f.read()\n",
    "\n",
    "def vectorize(docs, *args, **qsargs):\n",
    "    vectorizer = CountVectorizer(stop_words='english',  *args, **qsargs)\n",
    "    data = vectorizer.fit_transform(docs)\n",
    "    col_freq_filter = np.asarray(data.sum(axis=0) > 9)[0]\n",
    "    print \"col_freq_filter\", Counter(col_freq_filter).most_common()\n",
    "    print \"total words\", data[:, col_freq_filter].sum()\n",
    "    vocabulary = [t for _, t in sorted([(v, k) for k, v in vectorizer.vocabulary_.iteritems()])]\n",
    "    num_docs = len(docs)\n",
    "    vectorized_docs = [[] for _ in docs] \n",
    "    for row, col, count in zip(*sp.sparse.find(data)):\n",
    "        if not col_freq_filter[col]:\n",
    "            continue\n",
    "        for word in range(count):\n",
    "            vectorized_docs[row].append(vocabulary[col])\n",
    "    return vectorized_docs\n",
    "\n",
    "processed_abstracts = {}\n",
    "for _ in re.findall(\"Citation: (.+?)----\", raw_abstracts, re.DOTALL):\n",
    "    citation = _.split(\"\\n\")[0]\n",
    "    _ = re.findall(\"Abstract(.+)\", _, re.DOTALL)[0]\n",
    "    _ = _[2:]\n",
    "    \n",
    "    _ = re.sub(\"\\n\", \" \", _)\n",
    "    _ = re.sub(\"[ ]{2,}\", \" \", _)\n",
    "    _ = _.strip()\n",
    "    processed_abstracts[citation] = _\n",
    "\n",
    "print \"number of abstracts\", len(processed_abstracts)\n",
    "docs = vectorize(processed_abstracts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_file = \"preprocessed_bio_abstracts.txt\"\n",
    "with open(source_file, \"w\") as f:\n",
    "    f.write('\\n'.join([' '.join(doc) for doc in docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Shuyo Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha, beta, gamma = 1, 0.500, 1\n",
    "corpus = load_file(source_file)\n",
    "vocab = Vocabulary(excluds_stopwords=False)\n",
    "shuyo_docs = [vocab.doc_to_ids(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdplda = HDPLDA(alpha, beta, gamma, shuyo_docs, vocab.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Shuyo's code matches the original implementation, we should expect this to get 50-80 topics and a perplexity under 800. It's still running _slowly_. (This already ran 100 interations prior to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- K=27 p=850.036583\n",
      "- K=29 p=848.867273\n",
      "- K=27 p=848.869948\n",
      "- K=30 p=847.849667\n",
      "- K=31 p=846.927765\n",
      "- K=31 p=845.864101\n",
      "- K=29 p=845.398586\n",
      "- K=28 p=844.663386\n",
      "- K=29 p=843.789580\n",
      "- K=29 p=843.609830\n",
      "- K=28 p=842.799414\n",
      "- K=28 p=842.489442\n",
      "- K=29 p=841.660747\n",
      "- K=30 p=840.769906\n",
      "- K=29 p=840.322523\n",
      "- K=31 p=839.885603\n",
      "- K=30 p=839.915356\n",
      "- K=31 p=839.180617\n",
      "- K=28 p=838.798009\n",
      "- K=28 p=838.452270\n",
      "- K=28 p=838.054395\n",
      "- K=28 p=837.859003"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    hdplda.inference()\n",
    "    perplexity = hdplda.perplexity()\n",
    "    print \"- K=%d p=%f\" % (len(hdplda.using_k)-1, perplexity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ignore this for now\n",
    "def run_shuyo_trial(stopping_tolerance=0.01):\n",
    "    alpha, beta, gamma = 1, 0.500, 1\n",
    "    corpus = load_file(source_file)\n",
    "    vocab = Vocabulary(excluds_stopwords=False)\n",
    "    shuyo_docs = [vocab.doc_to_ids(doc) for doc in corpus]\n",
    "\n",
    "    hdplda = HDPLDA(alpha, beta, gamma, shuyo_docs, vocab.size())\n",
    "    perplexity = -1000000000\n",
    "    while True:\n",
    "        hdplda.inference()\n",
    "        old_perplexity = perplexity\n",
    "        perplexity = hdplda.perplexity()\n",
    "        print \"- K=%d p=%f\" % (len(hdplda.using_k)-1, perplexity)\n",
    "        if abs(old_perplexity - perplexity) / (perplexity + old_perplexity) < stopping_tolerance:\n",
    "            return {\"perplexity\": perplexity, \"num_topics\": len(hdplda.using_k)-1}\n",
    "            \n",
    "results = [run_shuyo_trial(.001) for _ in range(30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teh, et al, provide minimal information about their experiment with this data. The only mention of it is described below:\n",
    "\n",
    "<img src=\"./images/teh-abstracts.png\" width='600'>\n",
    "<img src=\"./images/teh-figs.png\" width='600'>\n",
    "\n",
    "When we run Shuyo's code until the perplexity changes by only 0.1%, we get a similar perplexity to Teh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Label Permutation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDP-LDA algorithm should be invariant under a variety of transformations of the input data. Thus, we can gain some confidence in Shuyo's implementation by verifying these invariant hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_docs(num_docs, vocab_size, num_topics, doc_len_dist, alpha=0.5):\n",
    "    docs = []\n",
    "    topics = sp.stats.dirichlet([alpha for _ in range(vocab_size)]).rvs(num_topics)\n",
    "    vocab = range(vocab_size)\n",
    "    for doc in range(num_docs):\n",
    "        doc_len = doc_len_dist()\n",
    "        topic_index = np.random.choice(range(len(topics)))\n",
    "        topic = topics[topic_index]\n",
    "        docs.append(list(np.random.choice(vocab, size=doc_len, p=topic)))\n",
    "    return docs\n",
    "     \n",
    "def rotate_labels(docs, vocab_size):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_docs.append([(word + 1) % vocab_size for word in doc])\n",
    "    return new_docs\n",
    "\n",
    "def rotate_docs(docs, vocab_size):\n",
    "    return docs[-1:] + docs[:-1]\n",
    "\n",
    "def shuffle_words(docs, vocab_size):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_doc = doc[:]\n",
    "        shuffle(new_doc)\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs\n",
    "\n",
    "def single_word(docs, vocab_size):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_docs.append([docs[0][0] for _ in doc])\n",
    "    return new_docs\n",
    "\n",
    "def string_labels(docs, vocab_size):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_docs.append([str(word) for word in doc])\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def trial(docs, vocab_size, alpha, beta, gamma, tol=0.001, max_steps=1000):\n",
    "    hdplda = HDPLDA(alpha=alpha, beta=beta, gamma=gamma, docs=docs, V=vocab_size)\n",
    "    perplexity = hdplda.perplexity()\n",
    "    for _ in range(max_steps):\n",
    "        hdplda.inference()\n",
    "        new_perplexity = hdplda.perplexity()\n",
    "        if abs(new_perplexity - perplexity) / (new_perplexity + perplexity) < tol:\n",
    "            print \".\",\n",
    "            return {\"perplexity\": new_perplexity, \"topics\": len(hdplda.using_k)-1}\n",
    "        perplexity = new_perplexity\n",
    "\n",
    "def run_trials(docs, vocab_size, doc_manipulation=None, alpha=1, beta=.5, gamma=1, num_trials=50):\n",
    "    if doc_manipulation is not None:\n",
    "        print doc_manipulation.__name__,\n",
    "        docs = doc_manipulation(docs, vocab_size)\n",
    "    results = [trial(docs, vocab_size, alpha=1, beta=0.5, gamma=1) for _ in range(num_trials)]\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size=5\n",
    "docs = generate_docs(num_docs=50, vocab_size=vocab_size, num_topics=5, doc_len_dist=lambda: sp.stats.poisson(500).rvs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . string_labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . shuffle_words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . rotate_labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . rotate_docs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . single_word . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"baseline\": run_trials(docs, vocab_size, None),\n",
    "    \"string_labels\": run_trials(docs, vocab_size, string_labels),\n",
    "    \"shuffle_words\": run_trials(docs, vocab_size, shuffle_words),\n",
    "    \"rotate_labels\": run_trials(docs, vocab_size, rotate_labels),\n",
    "    \"rotate_docs\": run_trials(docs, vocab_size, rotate_docs),\n",
    "    \"single_word\": run_trials(docs, vocab_size, single_word),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity and number of topics remains invariant under all transformations except the single_word transformation that actually changes the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>rotate_docs</th>\n",
       "      <th>rotate_labels</th>\n",
       "      <th>shuffle_words</th>\n",
       "      <th>single_word</th>\n",
       "      <th>string_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.101261</td>\n",
       "      <td>3.101178</td>\n",
       "      <td>3.101342</td>\n",
       "      <td>3.101319</td>\n",
       "      <td>1.000086</td>\n",
       "      <td>3.101484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.099200</td>\n",
       "      <td>3.100121</td>\n",
       "      <td>3.099553</td>\n",
       "      <td>3.099596</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>3.099717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.100883</td>\n",
       "      <td>3.100859</td>\n",
       "      <td>3.100984</td>\n",
       "      <td>3.101066</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>3.101029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.101202</td>\n",
       "      <td>3.101238</td>\n",
       "      <td>3.101241</td>\n",
       "      <td>3.101404</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>3.101489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.101669</td>\n",
       "      <td>3.101571</td>\n",
       "      <td>3.101609</td>\n",
       "      <td>3.101668</td>\n",
       "      <td>1.000086</td>\n",
       "      <td>3.102003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.103261</td>\n",
       "      <td>3.102345</td>\n",
       "      <td>3.102566</td>\n",
       "      <td>3.102330</td>\n",
       "      <td>1.000125</td>\n",
       "      <td>3.102935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        baseline  rotate_docs  rotate_labels  shuffle_words  single_word  \\\n",
       "count  50.000000    50.000000      50.000000      50.000000    50.000000   \n",
       "mean    3.101261     3.101178       3.101342       3.101319     1.000086   \n",
       "std     0.000763     0.000515       0.000551       0.000541     0.000006   \n",
       "min     3.099200     3.100121       3.099553       3.099596     1.000085   \n",
       "25%     3.100883     3.100859       3.100984       3.101066     1.000085   \n",
       "50%     3.101202     3.101238       3.101241       3.101404     1.000085   \n",
       "75%     3.101669     3.101571       3.101609       3.101668     1.000086   \n",
       "max     3.103261     3.102345       3.102566       3.102330     1.000125   \n",
       "\n",
       "       string_labels  \n",
       "count      50.000000  \n",
       "mean        3.101484  \n",
       "std         0.000682  \n",
       "min         3.099717  \n",
       "25%         3.101029  \n",
       "50%         3.101489  \n",
       "75%         3.102003  \n",
       "max         3.102935  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({k: [l['perplexity'] for l in v] for k, v in results.iteritems()}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>rotate_docs</th>\n",
       "      <th>rotate_labels</th>\n",
       "      <th>shuffle_words</th>\n",
       "      <th>single_word</th>\n",
       "      <th>string_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.060000</td>\n",
       "      <td>6.90000</td>\n",
       "      <td>6.840000</td>\n",
       "      <td>7.120000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>7.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.018402</td>\n",
       "      <td>1.05463</td>\n",
       "      <td>0.911603</td>\n",
       "      <td>0.872248</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        baseline  rotate_docs  rotate_labels  shuffle_words  single_word  \\\n",
       "count  50.000000     50.00000      50.000000      50.000000    50.000000   \n",
       "mean    7.060000      6.90000       6.840000       7.120000     1.020000   \n",
       "std     1.018402      1.05463       0.911603       0.872248     0.141421   \n",
       "min     6.000000      6.00000       6.000000       6.000000     1.000000   \n",
       "25%     6.000000      6.00000       6.000000       6.250000     1.000000   \n",
       "50%     7.000000      7.00000       7.000000       7.000000     1.000000   \n",
       "75%     8.000000      7.00000       7.000000       8.000000     1.000000   \n",
       "max    10.000000     10.00000      10.000000       9.000000     2.000000   \n",
       "\n",
       "       string_labels  \n",
       "count      50.000000  \n",
       "mean        7.120000  \n",
       "std         0.848528  \n",
       "min         6.000000  \n",
       "25%         7.000000  \n",
       "50%         7.000000  \n",
       "75%         8.000000  \n",
       "max         9.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({k: [l['topics'] for l in v] for k, v in results.iteritems()}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
